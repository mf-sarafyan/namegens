{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79288c9c",
   "metadata": {},
   "source": [
    "# MLP Exercise - fantasy name generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b4394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from ..src.layers import Linear, BatchNorm1d, Tanh\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbfcb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\namegens\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Fern_PC II\\.cache\\kagglehub\\datasets\\isaacbenge\\fantasy-for-markov-generator\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"isaacbenge/fantasy-for-markov-generator\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca3d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data, normalize strings \n",
    "import glob\n",
    "import pandas as pd \n",
    "allcsvs = []\n",
    "for f in glob.glob(path+\"/*.csv\"):#  + [\"./names_clean.csv\"]:\n",
    "    df = pd.read_csv(f, header=None)\n",
    "    series = (df[0].str.normalize('NFKD')\n",
    "                   .str.encode('ascii', errors='ignore')\n",
    "                   .str.decode('utf-8')\n",
    "                   .str.lower()\n",
    "                   .str.replace(r'[^\\w\\s]', '')\n",
    "                   .str.replace('/', '')\n",
    "                   .str.replace('(', '')\n",
    "                   .str.replace(')', '')\n",
    "                   .str.strip()\n",
    "                   \n",
    "                   )\n",
    "\n",
    "    allcsvs.append(series)\n",
    "\n",
    "# Adding names from names_clean.csv\n",
    "words = pd.concat(allcsvs).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7627eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mai', 'ba', 'binh', 'chi', 'cong', 'cuong', 'ha', 'hien']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(words))\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67386158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: \"'\", 3: '-', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z', 0: '.'}\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e92abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    \"\"\"\n",
    "    Encodes a string `s` into a list of integers using the `stoi` mapping.\n",
    "    \"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(t):\n",
    "    \"\"\"\n",
    "    Decodes a tensor or list `t` of integers into a string using the `itos` mapping.\n",
    "    \"\"\"\n",
    "    if hasattr(t, \"tolist\"):\n",
    "        t = t.tolist()\n",
    "    return ''.join([itos[i] for i in t])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6924e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([34932, 8, 8])  # (num_samples, block_size, block_size)\n",
      "Y shape: torch.Size([34932, 8])  # (num_samples, block_size)\n",
      "Each sample contains exactly 8 sequences of length 8\n",
      "X shape: torch.Size([4222, 8, 8])  # (num_samples, block_size, block_size)\n",
      "Y shape: torch.Size([4222, 8])  # (num_samples, block_size)\n",
      "Each sample contains exactly 8 sequences of length 8\n",
      "X shape: torch.Size([4381, 8, 8])  # (num_samples, block_size, block_size)\n",
      "Y shape: torch.Size([4381, 8])  # (num_samples, block_size)\n",
      "Each sample contains exactly 8 sequences of length 8\n",
      "Xtr shape: torch.Size([34932, 8, 8]), Ytr shape: torch.Size([34932, 8])\n",
      "✓ Dataset structure: (num_samples, block_size, block_size)\n",
      "✓ Each sample contains exactly 8 sequences of length 8\n",
      "\n",
      "Example: First sample's sequences (anchor position 0):\n",
      "  [ 0] .l......             -> e\n",
      "  [ 1] .le.....             -> s\n",
      "  [ 2] .les....             -> a\n",
      "  [ 3] .lesa...             -> .\n",
      "  [ 4] .lesa...             -> .\n",
      "  [ 5] .lesa...             -> .\n",
      "  [ 6] .lesa...             -> .\n",
      "  [ 7] lesa....             -> .\n",
      "\n",
      "Batch shapes after get_batch:\n",
      "  x_batch: torch.Size([16, 8])  # (batch_size * block_size, block_size)\n",
      "  y_batch: torch.Size([16])  # (batch_size * block_size,)\n",
      "\n",
      "✓ Each batch contains 2 samples, each with 8 sequences\n",
      "✓ All sequences have T=8 tokens - attention will work correctly!\n",
      "\n",
      "First few sequences in batch:\n",
      "  [ 0] .pa.....             -> i\n",
      "  [ 1] .pai....             -> n\n",
      "  [ 2] .pain...             -> m\n",
      "  [ 3] .painm..             -> e\n",
      "  [ 4] .painme.             -> w\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "g = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "# Helper function 1: Pad a string to block_size from the start\n",
    "def pad_start(word_chars, block_size):\n",
    "    \"\"\"\n",
    "    Pad a word (as list of token IDs) to block_size from the start with padding tokens (0).\n",
    "    \n",
    "    Args:\n",
    "        word_chars: List of token IDs representing the word\n",
    "        block_size: Target length\n",
    "    \n",
    "    Returns:\n",
    "        Padded list of length block_size\n",
    "    \"\"\"\n",
    "    padding_needed = block_size - len(word_chars)\n",
    "    if padding_needed > 0:\n",
    "        return [0] * padding_needed + word_chars\n",
    "    return word_chars\n",
    "\n",
    "# Helper function 2: Pad a word to block_size from the end (if shorter)\n",
    "def pad_end(word_chars, block_size):\n",
    "    \"\"\"\n",
    "    Pad a word (as list of token IDs) to block_size from the end with padding tokens (0).\n",
    "    \n",
    "    Args:\n",
    "        word_chars: List of token IDs representing the word\n",
    "        block_size: Target length\n",
    "    \n",
    "    Returns:\n",
    "        Padded list of length block_size\n",
    "    \"\"\"\n",
    "    padding_needed = block_size - len(word_chars)\n",
    "    if padding_needed > 0:\n",
    "        return word_chars + [0] * padding_needed\n",
    "    return word_chars[:block_size]  # Truncate if longer\n",
    "\n",
    "# Helper function 3: Build block_size sequences from a word starting at anchor position\n",
    "def build_block_sized_sequences(word_chars, anchor_pos, block_size):\n",
    "    \"\"\"\n",
    "    Build exactly block_size sequences starting from anchor position.\n",
    "    Each sequence starts with a single '.' token, then word characters.\n",
    "    \n",
    "    For anchor_pos=0 with word \"abc\" and block_size=8:\n",
    "        Sequence 0: ['.', 'a', ...] -> 'b'  (single '.' + word[0] + padding if needed)\n",
    "        Sequence 1: ['.', 'a', 'b', ...] -> 'c'  (single '.' + word[0:2] + padding)\n",
    "        Sequence 2: ['.', 'a', 'b', 'c', ...] -> '.'  (single '.' + word[0:3] + padding)\n",
    "        Sequence 3-7: ['.', 'a', 'b', 'c', '.', ...] -> padding (if word ends)\n",
    "    \n",
    "    For anchor_pos=1 with word \"abcdefghijk\" and block_size=8:\n",
    "        Sequence 0: ['.', 'a', 'b', ...] -> 'c'  (single '.' + word[0:2] + padding)\n",
    "        Sequence 1: ['.', 'a', 'b', 'c', ...] -> 'd'  (single '.' + word[0:3] + padding)\n",
    "        ...\n",
    "        Sequence 7: ['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g'] -> 'h'  (single '.' + word[0:7])\n",
    "    \n",
    "    Args:\n",
    "        word_chars: List of token IDs representing the word (with '.' at end)\n",
    "        anchor_pos: Starting position in the word (0-indexed)\n",
    "        block_size: Number of sequences to generate (each of length block_size)\n",
    "    \n",
    "    Returns:\n",
    "        X: List of block_size sequences, each of length block_size\n",
    "        Y: List of block_size target tokens\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # Get the '.' token ID (should be stoi['.'])\n",
    "    dot_token = stoi['.']\n",
    "    \n",
    "    # Build block_size sequences\n",
    "    for i in range(block_size):\n",
    "        # Each sequence starts with a single '.' token\n",
    "        context = [dot_token]\n",
    "        \n",
    "        # Determine how many word characters to include\n",
    "        # For anchor_pos=0: include word[0:i+1]\n",
    "        # For anchor_pos=1: include word[0:anchor_pos+i+1] (to include chars before anchor)\n",
    "        \n",
    "        if anchor_pos == 0:\n",
    "            # Simple case: start from beginning\n",
    "            chars_to_add = word_chars[0:min(i+1, len(word_chars))]\n",
    "        else:\n",
    "            # For anchor_pos > 0, include characters before anchor_pos\n",
    "            # Sequence i needs: '.' + word[0:anchor_pos+i+1]\n",
    "            end_char_idx = anchor_pos + i + 1\n",
    "            chars_to_add = word_chars[0:min(end_char_idx, len(word_chars))]\n",
    "        \n",
    "        context.extend(chars_to_add)\n",
    "        \n",
    "        # Pad at the end if needed (with padding token 0, not '.')\n",
    "        if len(context) < block_size:\n",
    "            # Pad with padding token (0) at the end\n",
    "            context.extend([0] * (block_size - len(context)))\n",
    "        elif len(context) > block_size:\n",
    "            # Take last block_size tokens (shouldn't happen, but safety)\n",
    "            context = context[-block_size:]\n",
    "        \n",
    "        X.append(context)\n",
    "        \n",
    "        # Target: the character at position anchor_pos + i + 1\n",
    "        target_pos = anchor_pos + i + 1\n",
    "        if target_pos < len(word_chars):\n",
    "            Y.append(word_chars[target_pos])\n",
    "        else:\n",
    "            # Beyond word length, predict padding token (0)\n",
    "            Y.append(0)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Refactored build_dataset function\n",
    "def build_dataset(words):\n",
    "    \"\"\"\n",
    "    Build dataset where each sample contains exactly block_size sequences.\n",
    "    Each sample is anchored at a different position in the word.\n",
    "    \n",
    "    For word \"abcdefghijk\" with block_size=8:\n",
    "    - Sample 1 (anchor=0): sequences from all padding -> 'a' up to 'a'...'h' -> 'i'\n",
    "    - Sample 2 (anchor=1): sequences from (7 padding + 'a') -> 'b' up to 'a'...'h' -> 'i'\n",
    "    - Sample 3 (anchor=2): sequences from (6 padding + 'ab') -> 'c' up to 'b'...'i' -> 'j'\n",
    "    - etc.\n",
    "    \n",
    "    Returns:\n",
    "        X: Tensor of shape (num_samples, block_size, block_size)\n",
    "        Y: Tensor of shape (num_samples, block_size)\n",
    "    \"\"\"\n",
    "    all_samples_X = []\n",
    "    all_samples_Y = []\n",
    "    \n",
    "    for w in words:\n",
    "        # Encode the word with '.' as end token\n",
    "        chars = [stoi[c] for c in w + '.']\n",
    "        \n",
    "        # Determine valid anchor positions\n",
    "        # For anchor_pos, we need to be able to predict up to chars[anchor_pos + block_size]\n",
    "        # So we need: anchor_pos + block_size < len(chars)\n",
    "        # Therefore: anchor_pos < len(chars) - block_size\n",
    "        \n",
    "        max_anchor = len(chars) - block_size\n",
    "        \n",
    "        # Create samples for each valid anchor position\n",
    "        for anchor_pos in range(max(0, max_anchor + 1)):\n",
    "            # Create a sample starting at this anchor position\n",
    "            sample_X, sample_Y = build_block_sized_sequences(chars, anchor_pos, block_size)\n",
    "            all_samples_X.append(sample_X)\n",
    "            all_samples_Y.append(sample_Y)\n",
    "        \n",
    "        # Handle words shorter than block_size\n",
    "        # Create one sample starting at position 0 with padding\n",
    "        if len(chars) <= block_size:\n",
    "            # Pad the word to block_size from the end\n",
    "            padded_chars = pad_end(chars, block_size)\n",
    "            sample_X, sample_Y = build_block_sized_sequences(padded_chars, 0, block_size)\n",
    "            all_samples_X.append(sample_X)\n",
    "            all_samples_Y.append(sample_Y)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = torch.tensor(all_samples_X, device=device)  # (num_samples, block_size, block_size)\n",
    "    Y = torch.tensor(all_samples_Y, device=device)  # (num_samples, block_size)\n",
    "    \n",
    "    print(f\"X shape: {X.shape}  # (num_samples, block_size, block_size)\")\n",
    "    print(f\"Y shape: {Y.shape}  # (num_samples, block_size)\")\n",
    "    print(f\"Each sample contains exactly {block_size} sequences of length {block_size}\")\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "def get_batch(split, batch_size):\n",
    "    \"\"\"\n",
    "    Generate a batch of samples.\n",
    "    \n",
    "    Each sample is (block_size, block_size) - block_size sequences of block_size tokens each.\n",
    "    Batch shape: (batch_size, block_size, block_size)\n",
    "    \n",
    "    Returns:\n",
    "        x: (batch_size * block_size, block_size) - flattened for processing\n",
    "        y: (batch_size * block_size,) - flattened targets\n",
    "    \"\"\"\n",
    "    data_X = Xtr if split == 'train' else Xdev if split == 'dev' else Xte\n",
    "    data_Y = Ytr if split == 'train' else Ydev if split == 'dev' else Yte\n",
    "    \n",
    "    # Generate indices on GPU to avoid CPU-GPU transfer overhead\n",
    "    ix = torch.randint(len(data_X), (batch_size,), device=device)\n",
    "    \n",
    "    # Each sample is (block_size, block_size), batch becomes (batch_size, block_size, block_size)\n",
    "    x = data_X[ix]  # (batch_size, block_size, block_size)\n",
    "    y = data_Y[ix]  # (batch_size, block_size)\n",
    "    \n",
    "    # Reshape to process all sequences: (batch_size * block_size, block_size)\n",
    "    # This allows the model to process all sequences from all samples in the batch\n",
    "    B, T_seq, T_tokens = x.shape\n",
    "    x = x.view(B * T_seq, T_tokens)  # (batch_size * block_size, block_size)\n",
    "    y = y.view(B * T_seq)  # (batch_size * block_size,)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Verify dataset structure\n",
    "print(f\"Xtr shape: {Xtr.shape}, Ytr shape: {Ytr.shape}\")\n",
    "print(f\"✓ Dataset structure: (num_samples, block_size, block_size)\")\n",
    "print(f\"✓ Each sample contains exactly {block_size} sequences of length {block_size}\\n\")\n",
    "\n",
    "# Show first sample's sequences\n",
    "print(\"Example: First sample's sequences (anchor position 0):\")\n",
    "first_sample_X = Xtr[0]  # (block_size, block_size)\n",
    "first_sample_Y = Ytr[0]  # (block_size,)\n",
    "for i in range(min(8, first_sample_X.shape[0])):\n",
    "    x_seq = first_sample_X[i].cpu().tolist()\n",
    "    y_token = first_sample_Y[i].item()\n",
    "    x_str = ''.join(['.' if tok == 0 else itos[tok] for tok in x_seq])\n",
    "    y_str = '.' if y_token == 0 else itos[y_token]\n",
    "    print(f\"  [{i:2d}] {x_str:20s} -> {y_str}\")\n",
    "\n",
    "# Test batch\n",
    "x_batch, y_batch = get_batch('train', 2)\n",
    "print(f\"\\nBatch shapes after get_batch:\")\n",
    "print(f\"  x_batch: {x_batch.shape}  # (batch_size * block_size, block_size)\")\n",
    "print(f\"  y_batch: {y_batch.shape}  # (batch_size * block_size,)\")\n",
    "print(f\"\\n✓ Each batch contains {2} samples, each with {block_size} sequences\")\n",
    "print(f\"✓ All sequences have T={block_size} tokens - attention will work correctly!\")\n",
    "print(f\"\\nFirst few sequences in batch:\")\n",
    "for i in range(min(5, len(x_batch))):\n",
    "    x_str = ''.join(['.' if tok == 0 else itos[tok] for tok in x_batch[i].cpu().tolist()])\n",
    "    y_str = '.' if y_batch[i].item() == 0 else itos[y_batch[i].item()]\n",
    "    print(f\"  [{i:2d}] {x_str:20s} -> {y_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec6d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported 50 words to dataset_inspection.json\n",
      "✓ Also created dataset_inspection.txt for quick viewing\n",
      "\n",
      "Files created:\n",
      "  - dataset_inspection.json (structured JSON)\n",
      "  - dataset_inspection.txt (human-readable text)\n"
     ]
    }
   ],
   "source": [
    "# Export dataset to JSON for inspection\n",
    "import json\n",
    "\n",
    "def export_dataset_to_json(X, Y, words_list, output_file, max_words=50):\n",
    "    \"\"\"\n",
    "    Export dataset to a structured JSON file for inspection.\n",
    "    \n",
    "    Args:\n",
    "        X: Tensor of shape (num_words, max_sequences, block_size)\n",
    "        Y: Tensor of shape (num_words, max_sequences)\n",
    "        words_list: List of original words (in same order as X/Y)\n",
    "        output_file: Path to output JSON file\n",
    "        max_words: Maximum number of words to export (for readability)\n",
    "    \"\"\"\n",
    "    dataset_export = []\n",
    "    \n",
    "    num_words_to_export = min(max_words, len(words_list))\n",
    "    \n",
    "    for word_idx in range(num_words_to_export):\n",
    "        word = words_list[word_idx]\n",
    "        word_X = X[word_idx].cpu().tolist()  # (max_sequences, block_size)\n",
    "        word_Y = Y[word_idx].cpu().tolist()  # (max_sequences,)\n",
    "        \n",
    "        sequences = []\n",
    "        for seq_idx in range(len(word_X)):\n",
    "            x_seq = word_X[seq_idx]\n",
    "            y_token = word_Y[seq_idx]\n",
    "            \n",
    "            # Skip padding sequences\n",
    "            if y_token < 0:\n",
    "                continue\n",
    "            \n",
    "            # Convert sequence to readable format\n",
    "            x_str = ''.join(['.' if tok == 0 else itos[tok] for tok in x_seq])\n",
    "            y_str = itos[y_token] if y_token >= 0 else '<PAD>'\n",
    "            \n",
    "            sequences.append({\n",
    "                \"sequence_index\": seq_idx,\n",
    "                \"input_sequence\": x_str,\n",
    "                \"input_tokens\": x_seq,\n",
    "                \"target_token\": y_str,\n",
    "                \"target_token_id\": int(y_token)\n",
    "            })\n",
    "        \n",
    "        if sequences:  # Only add if word has valid sequences\n",
    "            dataset_export.append({\n",
    "                \"word_index\": word_idx,\n",
    "                \"original_word\": word,\n",
    "                \"num_sequences\": len(sequences),\n",
    "                \"sequences\": sequences\n",
    "            })\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"dataset_info\": {\n",
    "                \"total_words\": len(words_list),\n",
    "                \"words_exported\": num_words_to_export,\n",
    "                \"max_sequences_per_word\": X.shape[1],\n",
    "                \"block_size\": block_size,\n",
    "                \"vocab_size\": vocab_size\n",
    "            },\n",
    "            \"words\": dataset_export\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Exported {num_words_to_export} words to {output_file}\")\n",
    "    return dataset_export\n",
    "\n",
    "# Export training set (first 50 words for inspection)\n",
    "train_words = words[:n1]  # Get the words used for training\n",
    "train_export = export_dataset_to_json(Xtr, Ytr, train_words, \"dataset_inspection.json\", max_words=50)\n",
    "\n",
    "# Also create a simpler text file for quick viewing\n",
    "with open(\"dataset_inspection.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Dataset Structure Inspection\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total words in training set: {len(train_words)}\\n\")\n",
    "    f.write(f\"Max sequences per word: {Xtr.shape[1]}\\n\")\n",
    "    f.write(f\"Block size (sequence length): {block_size}\\n\")\n",
    "    f.write(f\"Vocab size: {vocab_size}\\n\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for word_data in train_export[:20]:  # First 20 words\n",
    "        f.write(f\"\\nWord #{word_data['word_index']}: '{word_data['original_word']}'\\n\")\n",
    "        f.write(f\"  Sequences ({word_data['num_sequences']} total):\\n\")\n",
    "        for seq in word_data['sequences'][:10]:  # First 10 sequences per word\n",
    "            f.write(f\"    [{seq['sequence_index']:2d}] {seq['input_sequence']:20s} -> {seq['target_token']}\\n\")\n",
    "        if word_data['num_sequences'] > 10:\n",
    "            f.write(f\"    ... ({word_data['num_sequences'] - 10} more sequences)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✓ Also created dataset_inspection.txt for quick viewing\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - dataset_inspection.json (structured JSON)\")\n",
    "print(f\"  - dataset_inspection.txt (human-readable text)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c61e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        # Keep losses on GPU to avoid CPU sync overhead\n",
    "        losses = torch.zeros(eval_iters, device=device)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss  # Keep on GPU, no .item() here\n",
    "        # Only sync to CPU once at the end\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e276108",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Rebuild Dataset After Changing block_size!\n",
    "\n",
    "**If you changed block_size, you MUST re-run Cell 7 to rebuild the dataset!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..src import gpt_model\n",
    "# vocab_size calculated above\n",
    "\n",
    "# AGGRESSIVE GPU OPTIMIZATION SETTINGS:\n",
    "# 1. Longer sequences (block_size) - MOST IMPORTANT for transformers!\n",
    "\n",
    "# 2. Larger model dimensions\n",
    "n_embd = 64  # Increased from 64 - more compute per token\n",
    "n_head = 8  # Increased from 4 - more attention heads\n",
    "n_layer = 8   # Keep same\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 3e-3\n",
    "\n",
    "n_iter = 100000\n",
    "# 3. Larger batch size - fill that GPU!\n",
    "# With block_size=64, you can use larger batches\n",
    "# Start with 512, increase to 1024 if you have VRAM\n",
    "batch_size = 256  # Increased from 256 - more parallelism\n",
    "\n",
    "# Track losses on GPU, only sync to CPU periodically\n",
    "lossi_gpu = torch.zeros(n_iter // 100, device=device)  # Track training loss every 100 iterations\n",
    "val_lossi_gpu = torch.zeros(n_iter // 100, device=device)  # Track validation loss every 100 iterations\n",
    "lossi = []  # For final plotting\n",
    "val_lossi = []  # For final plotting\n",
    "ud = []\n",
    "\n",
    "\n",
    "model = gpt_model.BigramLanguageModel(vocab_size, n_embd, block_size, n_head, n_layer, dropout, device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Add weight decay for L2 regularization (prevents overfitting)\n",
    "weight_decay = 0.1  # L2 regularization strength\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler - reduce LR as training progresses\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_iter, eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8d59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 5060\n",
      "Total VRAM: 8.55 GB\n",
      "Allocated: 0.03 GB\n",
      "Cached: 0.04 GB\n"
     ]
    }
   ],
   "source": [
    "run_train = True  # Set to False to load pretrained model\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# IMPORTANT: Since block_size changed, you MUST rebuild the dataset!\n",
    "# Re-run Cell 7 to rebuild with the new block_size=64\n",
    "# The old dataset with block_size=8 won't work with the new model\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f943de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/100000 [00:12<144:15:42,  5.19s/iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0/100000:\n",
      "  Train loss: 3.1136\n",
      "  Val loss: 3.1109 (best: 3.1109)\n",
      "  Avg recent loss: 3.3703\n",
      "  Learning rate: 0.003000\n",
      "  Speed: 0.1 it/s\n",
      "  Elapsed: 12s\n",
      "  ETA: 339h 50m 10s\n",
      "  Patience: 0/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 5002/100000 [08:23<24:37:41,  1.07iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5000/100000:\n",
      "  Train loss: 1.5097\n",
      "  Val loss: 1.7670 (best: 1.7670)\n",
      "  Avg recent loss: 1.8549\n",
      "  Learning rate: 0.002982\n",
      "  Speed: 9.9 it/s\n",
      "  Elapsed: 8m 22s\n",
      "  ETA: 2h 39m 14s\n",
      "  Patience: 0/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 10001/100000 [16:22<25:00:38,  1.00s/iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10000/100000:\n",
      "  Train loss: 1.4378\n",
      "  Val loss: 1.7564 (best: 1.7564)\n",
      "  Avg recent loss: 1.7627\n",
      "  Learning rate: 0.002929\n",
      "  Speed: 10.2 it/s\n",
      "  Elapsed: 16m 22s\n",
      "  ETA: 2h 27m 20s\n",
      "  Patience: 0/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 15003/100000 [24:22<15:03:13,  1.57iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 15000/100000:\n",
      "  Train loss: 1.3963\n",
      "  Val loss: 1.7476 (best: 1.7476)\n",
      "  Avg recent loss: 1.7211\n",
      "  Learning rate: 0.002842\n",
      "  Speed: 10.3 it/s\n",
      "  Elapsed: 24m 21s\n",
      "  ETA: 2h 18m 3s\n",
      "  Patience: 0/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 20003/100000 [32:25<14:18:12,  1.55iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 20000/100000:\n",
      "  Train loss: 1.3665\n",
      "  Val loss: 1.7356 (best: 1.7356)\n",
      "  Avg recent loss: 1.6937\n",
      "  Learning rate: 0.002723\n",
      "  Speed: 10.3 it/s\n",
      "  Elapsed: 32m 25s\n",
      "  ETA: 2h 9m 41s\n",
      "  Patience: 0/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 25003/100000 [40:35<18:20:23,  1.14iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 25000/100000:\n",
      "  Train loss: 1.3402\n",
      "  Val loss: 1.7396 (best: 1.7356)\n",
      "  Avg recent loss: 1.6728\n",
      "  Learning rate: 0.002575\n",
      "  Speed: 10.3 it/s\n",
      "  Elapsed: 40m 35s\n",
      "  ETA: 2h 1m 45s\n",
      "  Patience: 1/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 25925/100000 [42:08<2:00:25, 10.25iter/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Gradient clipping - prevents exploding gradients and helps generalization\u001b[39;00m\n\u001b[32m     51\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m scheduler.step()  \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Update recent loss for progress bar (every 10 iterations)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\namegens\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:777\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    775\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m    780\u001b[39m torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if not run_train:\n",
    "    # Load the trained model\n",
    "    model.load_state_dict(torch.load(\"trained_gpt_model.pt\", map_location=device))\n",
    "    model.eval()\n",
    "else:\n",
    "    # Enable cuDNN benchmarking for consistent input sizes (faster)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Optional: Compile model for faster execution (PyTorch 2.0+)\n",
    "    # NOTE: torch.compile() requires Triton, which has limited Windows support\n",
    "    # If you're on Windows and get Triton errors, comment out the line below\n",
    "    # On Linux/Mac, you can uncomment this for 20-30% speedup\n",
    "    # model = torch.compile(model)\n",
    "    \n",
    "    # Helper function to format time\n",
    "    def format_time(seconds):\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        if hours > 0:\n",
    "            return f\"{hours}h {minutes}m {secs}s\"\n",
    "        elif minutes > 0:\n",
    "            return f\"{minutes}m {secs}s\"\n",
    "        else:\n",
    "            return f\"{secs}s\"\n",
    "    \n",
    "    # Track losses on GPU, sync to CPU less frequently\n",
    "    loss_tracker_idx = 0\n",
    "    \n",
    "    # Early stopping - stop if validation loss doesn't improve\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5  # Number of evaluations to wait for improvement\n",
    "    patience_counter = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Progress bar with time tracking\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(range(n_iter), desc=\"Training\", unit=\"iter\", \n",
    "                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    # Track recent loss for progress bar (update every 10 iterations)\n",
    "    recent_loss = None\n",
    "    \n",
    "    for i in pbar:\n",
    "        Xb, Yb = get_batch('train', batch_size)\n",
    "        logits, loss = model(Xb, Yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping - prevents exploding gradients and helps generalization\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        \n",
    "        # Update recent loss for progress bar (every 10 iterations)\n",
    "        if i % 10 == 0:\n",
    "            recent_loss = loss.item()\n",
    "            # Update progress bar with loss and time info\n",
    "            elapsed = time.time() - start_time\n",
    "            iters_per_sec = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining_iters = n_iter - (i + 1)\n",
    "            eta_seconds = remaining_iters / iters_per_sec if iters_per_sec > 0 else 0\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{recent_loss:.4f}',\n",
    "                'it/s': f'{iters_per_sec:.1f}',\n",
    "                'ETA': format_time(eta_seconds)\n",
    "            })\n",
    "        \n",
    "        # Track both training and validation loss every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            lossi_gpu[loss_tracker_idx] = loss.detach()\n",
    "            # Compute validation loss (quick evaluation)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xval, Yval = get_batch('val', batch_size)\n",
    "                _, val_loss = model(Xval, Yval)\n",
    "                val_lossi_gpu[loss_tracker_idx] = val_loss.detach()\n",
    "            model.train()\n",
    "            loss_tracker_idx += 1\n",
    "        \n",
    "        # Evaluation less frequently and with fewer iterations\n",
    "        if i % 5000 == 0:  # Check more frequently for early stopping\n",
    "            # Sync GPU losses to CPU for evaluation\n",
    "            recent_losses = lossi_gpu[:loss_tracker_idx].cpu().numpy()\n",
    "            avg_loss = recent_losses.mean() if len(recent_losses) > 0 else 0.0\n",
    "            \n",
    "            # Quick evaluation with fewer iterations\n",
    "            losses = estimate_loss(model, 100, batch_size)  # Reduced from 1000 to 100\n",
    "            elapsed = time.time() - start_time\n",
    "            iters_per_sec = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Early stopping check\n",
    "            val_loss = losses['val']\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), \"trained_gpt_model_best.pt\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    early_stop = True\n",
    "                    tqdm.write(f\"\\nEarly stopping triggered! No improvement for {patience} evaluations.\")\n",
    "                    tqdm.write(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                    break\n",
    "            \n",
    "            # Print detailed stats\n",
    "            tqdm.write(f\"\\nStep {i}/{n_iter}:\")\n",
    "            tqdm.write(f\"  Train loss: {losses['train']:.4f}\")\n",
    "            tqdm.write(f\"  Val loss: {losses['val']:.4f} (best: {best_val_loss:.4f})\")\n",
    "            tqdm.write(f\"  Avg recent loss: {avg_loss:.4f}\")\n",
    "            tqdm.write(f\"  Learning rate: {current_lr:.6f}\")\n",
    "            tqdm.write(f\"  Speed: {iters_per_sec:.1f} it/s\")\n",
    "            tqdm.write(f\"  Elapsed: {format_time(elapsed)}\")\n",
    "            remaining_iters = n_iter - (i + 1)\n",
    "            eta_seconds = remaining_iters / iters_per_sec if iters_per_sec > 0 else 0\n",
    "            tqdm.write(f\"  ETA: {format_time(eta_seconds)}\")\n",
    "            tqdm.write(f\"  Patience: {patience_counter}/{patience}\\n\")\n",
    "        \n",
    "    # Final sync: move all tracked losses to CPU for plotting\n",
    "    lossi = lossi_gpu[:loss_tracker_idx].cpu().numpy().tolist()\n",
    "    val_lossi = val_lossi_gpu[:loss_tracker_idx].cpu().numpy().tolist()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    if early_stop:\n",
    "        print(f\"\\nTraining stopped early after {format_time(total_time)}!\")\n",
    "        # Load best model\n",
    "        model.load_state_dict(torch.load(\"trained_gpt_model_best.pt\", map_location=device))\n",
    "    else:\n",
    "        print(f\"\\nTraining completed in {format_time(total_time)}!\")\n",
    "    print(f\"Average speed: {(i+1) / total_time:.1f} iterations/second\")\n",
    "    print(f\"Final best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \"trained_gpt_model.pt\")\n",
    "    \n",
    "    # Plot training and validation curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb71cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lossi) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Average losses over 100 iterations for smoother plot\n",
    "    if len(lossi) % 100 == 0:\n",
    "        train_loss_smooth = torch.tensor(lossi).view(-1, 100).mean(1)\n",
    "    else:\n",
    "        train_loss_smooth = torch.tensor(lossi)\n",
    "    if len(val_lossi) % 100 == 0:\n",
    "        val_loss_smooth = torch.tensor(val_lossi).view(-1, 100).mean(1)\n",
    "    else:\n",
    "        val_loss_smooth = torch.tensor(val_lossi)\n",
    "    \n",
    "    iterations = torch.arange(len(train_loss_smooth)) * 100\n",
    "    plt.plot(iterations, train_loss_smooth, label='Training Loss', linewidth=2)\n",
    "    plt.plot(iterations, val_loss_smooth, label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47f8223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "logits, _ = model(idx)\n",
    "logits = logits[:, -1, :]\n",
    "probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "idx_next = torch.multinomial(probs, num_samples=1, generator=g)  # (1, 1)\n",
    "idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac56d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "\n",
      "b\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "o\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator(device=device).manual_seed(2147483647 + 22)\n",
    "\n",
    "def generate(model):\n",
    "    # Generate one sequence at a time, stop if 0 is predicted\n",
    "    idx = torch.zeros((1, block_size - 1), dtype=torch.long, device=device)\n",
    "    idx = torch.randint(1, vocab_size, (1, 1), device=device)\n",
    "    out = []\n",
    "    for i in range(25):\n",
    "        logits, _ = model(idx)\n",
    "        logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1, generator=g)  # (1, 1)\n",
    "        next_token = idx_next.item()\n",
    "        out.append(next_token)\n",
    "        # Shift context window: drop first token, append new token\n",
    "        idx = torch.cat([idx[:, 1:], idx_next], dim=1)  # (1, block_size)\n",
    "       # print(next_token)\n",
    "        if next_token == 0:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "for _ in range(25):\n",
    "    out = generate(model)\n",
    "    print(''.join(itos[i] for i in out if i != 0))  # print string up to but not including 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d48c36",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m25\u001b[39m):\n\u001b[32m     21\u001b[39m     out = generate(model)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# print string up to but not including 0\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m25\u001b[39m):\n\u001b[32m     21\u001b[39m     out = generate(model)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(itos[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m out \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m))  \u001b[38;5;66;03m# print string up to but not including 0\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator(device=device).manual_seed(2147483647 + 22)\n",
    "\n",
    "def generate(model):\n",
    "    # Generate one sequence at a time, stop if 0 is predicted\n",
    "    idx = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "    out = [idx]\n",
    "    for i in range(25):\n",
    "        logits, _ = model(idx)\n",
    "        logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1, generator=g)  # (1, 1)\n",
    "        next_token = idx_next.item()\n",
    "        out.append(next_token)\n",
    "        # Shift context window: drop first token, append new token\n",
    "        idx = torch.cat([idx[:, 1:], idx_next], dim=1)  # (1, block_size)\n",
    "       # print(next_token)\n",
    "    return out\n",
    "\n",
    "for _ in range(25):\n",
    "    out = generate(model)\n",
    "    print(''.join(itos[i] for i in out if i != 0))  # print string up to but not including 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model forcing more words\n",
    "g = torch.Generator(device=device).manual_seed(2147483647 + 69)\n",
    "\n",
    "for _ in range(25):\n",
    "    has_space = False\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      # if we sample the special '.' token, replace with space if no space, otherwise break\n",
    "      if itos[ix] == ' ':\n",
    "        has_space = True\n",
    "      if ix == 0:\n",
    "        if has_space==False:\n",
    "            ix = stoi[' ']\n",
    "            has_space = True\n",
    "        else:\n",
    "            break\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model forcing \"the\"\n",
    "g = torch.Generator(device=device).manual_seed(2147483647 + 67)\n",
    "\n",
    "for _ in range(25):\n",
    "    has_the = False\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      # if we sample the special '.' token, replace with space if no space, otherwise break\n",
    "      if ix == 0:\n",
    "        if has_the==False:\n",
    "            context = context[5:] + [stoi[' '], stoi['t'], stoi['h'], stoi['e'], stoi[' ']]\n",
    "            has_the = True\n",
    "            out = out[5:] + [stoi[' '], stoi['t'], stoi['h'], stoi['e'], stoi[' ']]\n",
    "        else:\n",
    "            break\n",
    "      else:\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6447546",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)\n",
    "out[4:] + [stoi['t'], stoi['h'], stoi['e'], stoi[' ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05a0a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92be32f7",
   "metadata": {},
   "source": [
    "### from andrejs\n",
    "train 2.0193495750427246\n",
    "val 2.156538963317871\n",
    "### bigger network\n",
    "train 1.9244529008865356\n",
    "val 2.1407828330993652\n",
    "### adjusted sizes + more data\n",
    "train 1.642111897468567\n",
    "val 2.2296860218048096\n",
    "\n",
    "good names tho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0230e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
